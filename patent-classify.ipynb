{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! pip install --quiet --upgrade scikit-learn\n",
    "! pip install --quiet fuzzywuzzy\n",
    "! pip install --quiet python-levenshtein\n",
    "! pip install --quiet diskcache\n",
    "! pip install --quiet lime\n",
    "! pip install --quiet torch\n",
    "! pip install --quiet gcsfs\n",
    "! pip install --quiet xxhash\n",
    "! pip install --upgrade numba\n",
    "! pip install -e 'git://github.com/nandanrao/embed-software.git#egg=embed_software'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "! pip install --quiet spacy\n",
    "! python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import attr\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from validation.data import dot_train_data, get_soc_n, get_dictionary, indeed_test_data\n",
    "from embed_software.preprocess import *\n",
    "from embed_software.utils import get_embeddings, embed_docs\n",
    "from classification.embedding import PreEmbeddedVectorizer\n",
    "from validation.scoring import bubbleup_score, BubbleUpMixin\n",
    "\n",
    "pd.set_option('max_colwidth',50)\n",
    "pd.set_option('display.width', 700)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 100000\n",
    "SOC_LEVEL = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = dot_train_data(SOC_LEVEL, include_dot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tasks = pd.read_csv('tasks.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19530,), (19530, 7))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, tasks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class BubbleUpLogisticRegression(BubbleUpMixin, LogisticRegression):\n",
    "    pass\n",
    "\n",
    "model = Pipeline([\n",
    "    ('sentencespace_200_patent_descriptions', PreEmbeddedVectorizer('./patent-descriptions/models/sentencespace-200', cache_dir='patent-description-cache-dir', chunk_size=1000)),\n",
    "    ('lr', BubbleUpLogisticRegression(C=2., solver='lbfgs', class_weight='balanced', multi_class=\"multinomial\", n_jobs=-1).set_bubbles(3))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mapping = pd.read_csv('./patent-descriptions/hbs_pat_ipc_mapping.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "patent_ids = set(mapping.patent.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from itertools import islice\n",
    "import sys\n",
    "from itertools import takewhile, islice, count\n",
    "from validation.scoring import get_top_soc_n_preds, get_soc_n_preds, make_code_lookup\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "def chunk(n, it):\n",
    "    src = iter(it)\n",
    "    return takewhile(bool, (list(islice(src, n)) for _ in count(0)))\n",
    "\n",
    "def cast(i):\n",
    "    try: \n",
    "        return int(i)\n",
    "    except ValueError:\n",
    "        return i\n",
    "\n",
    "def format_preds(d, socs, probs):\n",
    "    a,b = [(pd.DataFrame(df) \n",
    "            .assign(idx = d.id) \n",
    "            .melt(id_vars = ['idx'], value_name = key) \n",
    "            .drop(columns = 'variable')\n",
    "            .sort_values('idx')\n",
    "            .reset_index(drop=True)\n",
    "            .set_index('idx'))\n",
    "           for df,key in [(socs, 'soc'), (probs, 'prob')]]\n",
    "\n",
    "    df = pd.concat([a,b], 1).reset_index().rename(columns = {'idx': 'patent'})\n",
    "    return df\n",
    "\n",
    "def make_predictions(model, mapping, lines):\n",
    "    d = pd.DataFrame(lines, columns = ['id', 'description'])\n",
    "    preds = model.predict_proba(d.description)\n",
    "    preds_df = pd.DataFrame(preds)\n",
    "    preds_df.columns = model.classes_    \n",
    "    preds_socs, preds_probs = get_top_soc_n_preds(preds_df, 3, 5, True)    \n",
    "    return mapping.merge(format_preds(d, preds_socs, preds_probs), how = 'inner', on = 'patent')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = PreEmbeddedVectorizer('../abstracts-ss-100', cache_dir='patent-abstracts-cache-dir', chunk_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "with open('patent-descriptions/processed/details.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    lines = ([cast(l[0]), l[1]] for l in reader)\n",
    "    lines = (l for l in lines if l[0] in patent_ids)\n",
    "    lines = chunk(100000, lines)  \n",
    "    pred_dfs = [make_predictions(model, mapping, list(c)) for c in lines]\n",
    "\n",
    "    # pred_dfs = Parallel(n_jobs = -1)(delayed(make_predictions)(model, mapping, list(c)) for c in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "all_preds = pd.concat(pred_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "all_preds.to_csv('patent-descriptions/patent-preds-all.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# ABSTRACTS - NEAREST SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import islice\n",
    "from json import JSONDecodeError\n",
    "\n",
    "def loads(l):\n",
    "    try:\n",
    "        return json.loads(l)\n",
    "    except JSONDecodeError:\n",
    "        return { 'id': None }\n",
    "\n",
    "with open('../patent-abstracts/abstracts') as f:\n",
    "    dat = ((i, loads(l)) for i,l in enumerate(f) if i != 0)\n",
    "    dat = ((d['id'], d.get('abstract')) for i,d in dat)\n",
    "    dat = list(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from embed_software.preprocess import claims_processor, readme_processor, Preprocessor, tokenizer\n",
    "\n",
    "\n",
    "sentence = re.compile(r\"\\.\\s+\")\n",
    "\n",
    "def tokenize(p, min_tokens):\n",
    "    li_tokens = tokenizer(p)\n",
    "    if len(li_tokens) < min_tokens:\n",
    "        return None\n",
    "    return ' '.join(li_tokens)  \n",
    "\n",
    "def preprocessor(og, min_tokens = 3):\n",
    "    sents = sentence.split(og)\n",
    "    sents = [(s.strip(), claims_processor(s, numbers=True)) for s in sents]\n",
    "    \n",
    "    sents = [(s, tokenize(p, min_tokens)) for s,p in sents]\n",
    "    sents = [(s, p) for s,p in sents if p is not None]\n",
    "    originals, processed = zip(*sents)\n",
    "\n",
    "    # each a list of strings.\n",
    "    return originals, processed\n",
    "\n",
    "def process(preprocessor, i, text):\n",
    "    if text is None:\n",
    "        return (i, text, text)\n",
    "\n",
    "    try:\n",
    "        originals, processed = preprocessor(text)\n",
    "        return (i, originals, processed)\n",
    "    except ValueError: \n",
    "        return (i, None, None)\n",
    "\n",
    "def process_abstracts_chunk(preprocessor, dat):\n",
    "    processed = [process(preprocessor, i, t) for i,t in dat]\n",
    "    return [(i,og,p) for i,og,p in processed if p]\n",
    "\n",
    "def process_abstracts(preprocessor, dat, chunks = 32):\n",
    "    chunks = chunk(chunks, dat)\n",
    "    processed = Parallel(n_jobs=-1)(delayed(process_abstracts_chunk)(preprocessor, c) for c in chunks)\n",
    "    # processed = (process_abstracts_chunk(preprocessor, c) for c in chunks)\n",
    "    return [y for x in processed for y in x]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "from classification.embedding import WordEmbeddingVectorizer\n",
    "\n",
    "@njit(parallel=True)\n",
    "def get_nearest(embedded_tasks, embedded_sents, K = 1):\n",
    "    N = embedded_sents.shape[0]\n",
    "    A,D = np.zeros((N, K)), np.zeros((N, K))\n",
    "    for i in np.arange(N):\n",
    "        sim = embedded_tasks.dot(embedded_sents[i])\n",
    "        top_idxs = np.argsort(sim)[-K:]\n",
    "        for j,k in enumerate(top_idxs):\n",
    "            A[i,j] = k\n",
    "            D[i,j] = sim[k]\n",
    "    return A,D\n",
    "\n",
    "def make_results(idx, A, D, tasks, y_train, ids, ogs, sents, include_sents = True):\n",
    "    ii = np.argwhere(idx)[:, 0].flatten()\n",
    "    df = pd.DataFrame({\n",
    "        'patent_id': ids.iloc[ii].values,\n",
    "        'task_id': tasks.loc[A[idx]]['Task ID'].values,\n",
    "        'task_soc': y_train[A[idx]].values,\n",
    "        'distance': 1 - D[idx],\n",
    "        'sent_id': sents.iloc[ii].index.values\n",
    "    })\n",
    "    if include_sents:\n",
    "        df['abstract_sentence'] = ogs.iloc[ii].values\n",
    "        df['task_sentence'] = tasks.loc[A[idx], 'Task'].values\n",
    "    return df\n",
    "\n",
    "def create_sentence_matches(X_train, y_train, tasks, patents):\n",
    "    dat = [(r.patent, r.abstract) for i,r in patents.iterrows()]\n",
    "\n",
    "    lines = process_abstracts(preprocessor, dat, 32)\n",
    "    lines = [(i,ogg,pp) for i,og,p in lines for ogg,pp in zip(og, p)]\n",
    "\n",
    "    ids, ogs, sents = zip(*lines)\n",
    "    ids, ogs, sents = pd.Series(ids), pd.Series(ogs), pd.Series(sents)\n",
    "\n",
    "    vectorizer = WordEmbeddingVectorizer('../patent-abstracts/abstracts-ss-100.tsv', sep = '\\t', cache_dir=None, chunk_size=1000, max_workers = 1)\n",
    "\n",
    "    embedded_tasks = vectorizer.fit_transform(X_train)\n",
    "    embedded_sents = vectorizer.fit_transform(sents)\n",
    "\n",
    "    A, D = get_nearest(embedded_tasks, embedded_sents, 8)\n",
    "    \n",
    "    all_idx = D <= 1.0\n",
    "\n",
    "    return make_results(all_idx, A, D, tasks, y_train, ids, ogs, sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Filter dat by sample!\n",
    "\n",
    "mapping = pd.read_csv('./patent-descriptions/hbs_pat_ipc_mapping.csv', sep='\\t')\n",
    "mapping['patent'] = mapping.patent.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def count_sents(a):\n",
    "    try: \n",
    "        return len(a.split('. '))\n",
    "    except AttributeError:\n",
    "        return 0\n",
    "\n",
    "def is_cutoff(a):\n",
    "    return a[-1] != '.' if a else True\n",
    "\n",
    "patents = pd.DataFrame(dat, columns=['patent', 'abstract']) \\\n",
    "            .drop_duplicates('patent') \\\n",
    "            .merge(mapping, on='patent', how='inner') \\\n",
    "            .pipe(lambda df: df.assign(num_sents = df.abstract.map(count_sents))) \\\n",
    "            .pipe(lambda df: df.assign(cutoff = df.abstract.map(is_cutoff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "close_sents = pd.read_csv('./patent-neighbors/week1/super_close_matches.csv')\n",
    "close_idxs = close_sents.patent_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "valid_patents = patents[(patents.appyear < 2007) & \n",
    "                        (patents.appyear > 1973) & \n",
    "                        (patents.cutoff == False) ]\n",
    "\n",
    "sampled_patents = valid_patents \\\n",
    "    [~valid_patents.patent.isin(close_idxs)] \\\n",
    "    .groupby(['class', 'appyear']) \\\n",
    "    .apply(lambda df: df.sample(5, random_state=SEED) if df.shape[0] > 5 else None) \\\n",
    "    .reset_index(drop = True)\n",
    "\n",
    "close_patents = valid_patents[valid_patents.patent.isin(close_sents.patent_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33min 43s, sys: 1h 25min 56s, total: 1h 59min 39s\n",
      "Wall time: 4min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "results = create_sentence_matches(X_train, y_train, tasks, sampled_patents)\n",
    "close_results = create_sentence_matches(X_train, y_train, tasks, close_patents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372016, 7)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133656, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Selecting for MTurk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from toolz import curry\n",
    "\n",
    "@curry\n",
    "def add_nested_local_id(nested, name, df):\n",
    "    lookup = { s:i+1 for i,s in \n",
    "               enumerate(df[nested].unique()) }\n",
    "\n",
    "    df[name] = df[nested].map(lookup)\n",
    "    return df\n",
    "\n",
    "@curry\n",
    "def add_local_id(name, df, randomize=True):\n",
    "    idx = np.arange(df.shape[0]) + 1\n",
    "    if randomize:\n",
    "        np.random.shuffle(idx)\n",
    "    return df.assign(**{name: idx}).sort_values(name)\n",
    "\n",
    "\n",
    "def split_out_results(results, patents, group):\n",
    "    patents = patents \\\n",
    "        .pipe(lambda df: df.assign(abstract = df.abstract.map(lambda a: ' '.join(a.split()).strip()))) \\\n",
    "        .rename(columns = {'appyear': 'year', 'patent': 'patent_id'}) \\\n",
    "        [['patent_id', 'abstract', 'year', 'class']] \\\n",
    "        .assign(classifier_group = group)\n",
    "\n",
    "    ided = results[['patent_id', 'sent_id', 'abstract_sentence', \n",
    "                'task_id', 'distance', 'task_sentence']] \\\n",
    "                .groupby(['patent_id']) \\\n",
    "                .apply(add_nested_local_id('sent_id', 'sent_index')) \\\n",
    "                .reset_index(drop=True)\n",
    "\n",
    "    # Take only first 5 sentences\n",
    "    # ided = ided[ided.sent_index <= 5]\n",
    "\n",
    "    sentences = ided.drop_duplicates('sent_id') \\\n",
    "                    .drop(columns = ['task_id', 'distance', 'task_sentence', 'sent_id'])\n",
    "\n",
    "    mturk_tasks = ided \\\n",
    "        .groupby(['patent_id', 'sent_index']) \\\n",
    "        .apply(add_local_id('task_index')) \\\n",
    "        .reset_index(drop=True) \\\n",
    "        .drop(columns = ['abstract_sentence', 'task_id', 'sent_id'])\n",
    "\n",
    "    return patents, sentences, mturk_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def reformat_close_sents(sents, patents):\n",
    "    d = sents.merge(patents[['patent', 'abstract']], how='left', left_on='patent_id', right_on='patent')[['patent_id', 'abstract']]\n",
    "    d = d[d['abstract'].notna()]\n",
    "    dat = [(r.patent_id, r.abstract) for i,r in d.iterrows()]\n",
    "    return sents, dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_group_idxs(per_worker):\n",
    "    idxs = [0]*per_worker\n",
    "    for i in [3,6,9,12,15,21]:\n",
    "        idxs[i] = 1\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def assign_workers(patent_list, group_idxs, num_workers, per_worker, ):\n",
    "    out = []\n",
    "\n",
    "    vals = [p.values.tolist() for p in patent_list]\n",
    "    curr_vals = [v.copy() for v in vals]\n",
    "\n",
    "    for i in np.arange(num_workers):\n",
    "        i = i+1\n",
    "        for j in np.arange(per_worker):\n",
    "            gidx = group_idxs[j]\n",
    "            if len(curr_vals[gidx]) == 0:\n",
    "                curr_vals[gidx] = vals[gidx].copy()\n",
    "            out.append((i, curr_vals[gidx].pop()))\n",
    "\n",
    "    return pd.DataFrame(out, columns = ['worker_id', 'patent_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 26s, sys: 1.05 s, total: 3min 27s\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "group_a = split_out_results(results, sampled_patents, 'A')\n",
    "group_b = split_out_results(close_results, close_patents, 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 92 ms, sys: 0 ns, total: 92 ms\n",
      "Wall time: 91.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from math import floor\n",
    "\n",
    "a_size = floor(24*40 / 3)\n",
    "b_size = floor(6*40 / 3)\n",
    "\n",
    "group_a_patents = group_a[0] \\\n",
    "    .groupby('class') \\\n",
    "    .apply(lambda df: df.sample(5)) \\\n",
    "    .reset_index(drop=True) \\\n",
    "    .sample(a_size).patent_id\n",
    "\n",
    "group_b_patents = group_b[0].sample(b_size).patent_id\n",
    "\n",
    "workers = assign_workers([group_a_patents, group_b_patents], \n",
    "                         make_group_idxs(30), \n",
    "                         40, \n",
    "                         30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patents, sentences, mturk_tasks = [pd.concat(t) for t in zip(*[group_a, group_b])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "patents.to_csv('mturk-data/abstracts.csv', index=False)\n",
    "# sentences.to_csv('mturk-data/sentences.csv', index=False)\n",
    "# mturk_tasks.to_csv('mturk-data/tasks.csv', index=False)\n",
    "workers.to_csv('mturk-data/workers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "results = pd.read_csv('patent-neighbors/week0/abstracts-neighbor-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "i = 7\n",
    "\n",
    "def get_subj(sent):\n",
    "    subjs = [t for t in nlp(sent) \n",
    "             if t.dep_ == 'nsubj']\n",
    "    words = [str(s) for s in subjs]\n",
    "    subj = ','.join(words) if words else None\n",
    "    return subj\n",
    "\n",
    "results = results.assign(subj = [get_subj(sent) for sent in results.abstract_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "results.to_csv('abstracts-neighbor-sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Pipeline([('sentencespace_100_us', PreEmbeddedVectorizer('../abstracts-ss-100', 100, cache_dir='embed_cache')),\n",
    "                  ('knn', KNeighborsClassifier(1, n_jobs=-1))])\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def _get_soc_n(df, n):\n",
    "    return (df.T\n",
    "            .reset_index()\n",
    "            .pipe(lambda df: df.assign(soc = df['index'].map(lambda i: str(i)[0:n])))\n",
    "            .set_index('soc')\n",
    "            .drop('index', 1)\n",
    "            .groupby('soc').sum().T)\n",
    "\n",
    "\n",
    "def get_pred(model, X):\n",
    "    vals = model.predict_proba(X)\n",
    "    df = pd.DataFrame(vals)\n",
    "    df.columns = model.classes_\n",
    "    n=3\n",
    "    return _get_soc_n(df, n)\n",
    "\n",
    "class UpscaleModel(LogisticRegression):\n",
    "    def predict_soc_n(self, X, n):\n",
    "        preds = self.predict_proba(X)\n",
    "        df = pd.DataFrame(preds)\n",
    "        df.columns = labels\n",
    "        return self._get_soc_n(df, n)\n",
    "    \n",
    "    \n",
    "def make_title_lookup(path, N):\n",
    "    dot_codes = get_dictionary('', N).groupby('soc').first()\n",
    "    d = dot_codes[f'desc_soc{N}'].to_dict()\n",
    "    def lookup(code):\n",
    "        try:\n",
    "            return d[int(code)]\n",
    "        except KeyError:\n",
    "            return code\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "labels = np.unique(y_train)\n",
    "lookup = make_title_lookup('', 3)\n",
    "\n",
    "def get_soc_n(df, n):\n",
    "    return (df.T\n",
    "            .reset_index()\n",
    "            .pipe(lambda df: df.assign(soc = df['index'].map(lambda i: str(i)[0:n])))\n",
    "            .set_index('soc')\n",
    "            .drop('index', 1)\n",
    "            .groupby('soc')\n",
    "            .sum().T\n",
    "            .idxmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def print_preds(model, labels, target):\n",
    "    preds = model.predict_proba(target)\n",
    "    df = pd.DataFrame(preds)\n",
    "    df.columns = labels\n",
    "\n",
    "    res = [(lookup(cl),li) for cl,li in zip(get_soc_n(df, 3).values, target)]\n",
    "\n",
    "    for title, desc in res:\n",
    "        print(title)\n",
    "        print(desc)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print_preds(model, labels, lines[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print_preds(model, labels, lines[1000:1025])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print_preds(model, labels, lines[5000:5025])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print_preds(model, labels, lines[5000:5025])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print_preds(model, labels, lines[12000:12050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "l = pd.concat([lines[:50], lines[1000:1050], lines[10000:10050], lines[-50:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "a = embed_docs('../abstracts-ss-100', '\\n'.join(l))\n",
    "\n",
    "nn = NearestNeighbors()\n",
    "nn.fit(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def print_neighbors(nn, a, i):\n",
    "    _, idxs = nn.kneighbors(a[i].reshape(1,-1), n_neighbors = 5)\n",
    "    for i in idxs:\n",
    "        print(l.values[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/opt/conda/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "patent-classify.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
